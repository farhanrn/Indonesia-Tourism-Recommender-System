# -*- coding: utf-8 -*-
"""Salinan Indonesian Tourism Recommender System

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1f6FxqN9l00ZaaMmM4dVYXfIT1KL-9m4u

## System Configuration
"""

!pip install sastrawi
!pip install kaggle
!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

!rm -rf idonesian_tourism/
!kaggle datasets download -d aprabowo/indonesia-tourism-destination -p /content/dataset --unzip

"""## Import Library yang akan digunakan"""

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from matplotlib.colors import LinearSegmentedColormap
import re
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import Pipeline

from Sastrawi.Stemmer.StemmerFactory import StemmerFactory
from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory
from sklearn.metrics.pairwise import cosine_similarity
from io import BytesIO
import base64
import statsmodels.api as sm # Add this import to your existing imports

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras import layers
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense

"""# ⭐ Data Understanding ⭐

# Place Tourism

| Nama Kolom | Tipe Data | Deskripsi |
|---|---|---|
| Place_Id | int64 | ID unik untuk setiap tempat |
| Place_Name | object | Nama tempat wisata |
| Description | object | Deskripsi tempat |
| Category | object | Kategori tempat (misalnya, Alam, Budaya) |
| City | object | Kota tempat wisata berada |
| Price | float64 | Harga atau biaya masuk |
| Rating | float64 | Rating rata-rata tempat |
| Time_Minutes | float64 | Estimasi waktu kunjungan |
| Coordinate | object | Koordinat latitude dan longitude |
| Lat | float64 | Koordinat latitude |
| Long | float64 | Koordinat longitude |

### Reading Data
"""

place_tourism = pd.read_csv('/content/dataset/tourism_with_id.csv')
place_tourism.head()

"""### Assesing Data"""

print("Bentuk Data :", place_tourism.shape)
print(10*"=")
# Data information
print("Informasi pada setiap Kolom pada dataset place_toursim :")
place_tourism.info()
print(10*"=")
# Data Duplicated
print("Jumlah Duplikat Data :")
place_tourism.duplicated().sum()
# Data Null
print("Jumlah Null Data :")
place_tourism.isna().sum()

print("Jumlah Unique Data :")
place_tourism.nunique()

"""Dari hasil assessing data pada dataset **`place_tourism`** diperoleh bahwa :

- Tidak ada data duplikasi pada dataset
- Terdapat missing value pada kolom Time Minutes sebanyak 232 dan Unnamed :11 sebanyak 437 dan Unnamed :12 sebanyak 437. sehingga perlu dilakukan cleaning dengan menggunakan metode imputasi dengan metode Mean

# Rating

### Reading Data
"""

rating = pd.read_csv('/content/dataset/tourism_rating.csv')
rating.head(3)

"""### Assesing Data"""

print("Bentuk Data :", rating.shape)
print(10*"=")
# Data information
print("Informasi pada setiap Kolom pada dataset place_toursim :")
rating.info()
print(10*"=")
# Data Duplicated
print("Jumlah Duplikat Data :")
rating.duplicated().sum()
# Data Null
print("Jumlah Null Data :")
rating.isna().sum()

"""Dataset tidak memiliki missing value pada ketiga kolom"""

print("Jumlah Unique Data :")
rating.nunique()

"""Pada dataset Rating, diketahui bahwa raw data sebanyak 10000 dan memiliki 3 variabel `User_id`, `place_id` dan `Place_Ratings` tidak memiliki missing value dan  tidak memiliki data yang duplikat

- User_Id memiliki 300 data unique
- Place_id memiliki 437 data unique
- Place_Ratings memiliki 5 data unique

# User

### Reading Data
"""

user = pd.read_csv('/content/dataset/user.csv')
user.head(3)

"""### Assesing Data"""

print("Bentuk Data :", user.shape)
print(10*"=")
# Data information
print("Informasi pada setiap Kolom pada dataset place_toursim :")
user.info()
print(10*"=")
# Data Duplicated
print("Jumlah Duplikat Data :")
user.duplicated().sum()
# Data Null
print("Jumlah Null Data :")
user.isna().sum()

"""Dataset user memiliki 300 baris dan 3 kolom. Dataset ini tidak memiliki duplicated data dan tidak memiliki missing value

# ⭐ Data Preprocessing ⭐

## Handling Missing Value in Dataset Place Tourism
"""

place_tourism.isnull().sum()

"""Selanjutnya, mengatasi missing value dengan cara metode imputasi Mean pada kolom Time Minutes"""

# Imputation to Time_Minutes column
place_tourism['Time_Minutes'] = place_tourism['Time_Minutes'].fillna(place_tourism['Time_Minutes'].mean())

print("Setelah Dilakukan Filling Missing Value")
place_tourism.isnull().sum()

"""Kita tidak menghilangkan missing value pada Kolom Unnamed: 11 dan Unnamed: 12 karena akan di drop"""

place_tourism.head(4)

"""## Menghilangkan Kolom yang Tidak Diinginkan

Menghilangkan Unnamed: 11', 'Unnamed: 12 dengan function .drop()
"""

place_tourism.drop(['Unnamed: 11', 'Unnamed: 12'], axis=1, inplace=True)

place_tourism.head()

"""Menyimpan dataset yang bersih csv menjadi `place_tourism_clean.csv`"""

# Save Cleaned Data of place_tourism
place_tourism.to_csv('place_tourism_clean.csv', index=False)

"""# ⭐ Exploratory Data Analysis ⭐

## Univariate EDA on **place_tourism** dataset
"""

place_tourism

"""### Place Name
Ada berapa lokasi wisata dalam dataset?
"""

print("Total lokasi wisata :",place_tourism['Place_Name'].nunique())

"""### Category
Ada berapa lokasi wisata dalam dataset?
"""

gradient_blue = LinearSegmentedColormap.from_list("gradient_blue", ["#add8e6", "#00008b"])
plt.figure(figsize=(8, 3))
category_counts = place_tourism['Category'].value_counts().sort_values(ascending=True)

# Buat horizontal bar plot dengan gradient blue color
bars = plt.barh(
    category_counts.index,
    category_counts.values,
    color=gradient_blue([i / len(category_counts) for i in range(len(category_counts))])
)

for bar in bars:
    plt.text(bar.get_width(), bar.get_y() + bar.get_height() / 2,
             str(int(bar.get_width())),
             va='center',
             ha='left',
             fontsize=9,
             color='black')

plt.suptitle('Distribusi Data Kategori Tempat Wisata', fontsize=16)
plt.xlabel('Jumlah', fontsize=12)
#plt.ylabel('Kategori', fontsize=12)

# Sesuaikan layout dan tampilkan
plt.tight_layout()
plt.show()

"""Pada distribusi Data Kategori tempat Wisata, **Tempat Hiburan** menempati posisi teratassebanyak 135 kemudian disusul oleh **Budaya** sebesar 117. Adapun distribusi pada Kategori **Pusat Perbelanjaan** adalah sebanyak 15 yang menempati posisi terakhir

### City
Ada berapa lokasi kota wisata dalam dataset?
"""

gradient_blue = LinearSegmentedColormap.from_list("gradient_blue", ["#add8e6", "#00008b"])
plt.figure(figsize=(8, 5))  # Ubah figsize untuk tampilan yang lebih baik
city_counts = place_tourism['City'].value_counts().sort_values(ascending=True)

# Buat vertical bar plot dengan gradient blue color
bars = plt.bar(
    city_counts.index,  # Kota di sumbu x
    city_counts.values,  # Jumlah di sumbu y
    color=gradient_blue([i / len(city_counts) for i in range(len(city_counts))])
)

for bar in bars:
    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height(),
             str(int(bar.get_height())),
             va='bottom',
             ha='center',
             fontsize=9,
             color='black')

plt.suptitle('Distribusi Data Kota Tempat Wisata', fontsize=16)
#plt.xlabel('Kota', fontsize=12)  # Label sumbu x
plt.ylabel('Jumlah', fontsize=12)  # Label sumbu y

# Rotasi label sumbu x agar mudah dibaca
plt.xticks(rotation=45, ha='right')

# Sesuaikan layout dan tampilkan
plt.tight_layout()
plt.show()

"""**Yogyakarta** memiliki jumlah terbanyak sebesar 126, selisih 2 dengan **Bandung** yang sebesar 124. Kemudian **Surabaya** menjadi yang terakhir sebesar 46 saja

### Pesebaran harga
Persebaran Data harga Pada wisata
"""

plt.figure(figsize=(5, 3))
sns.histplot(place_tourism['Price'], kde=True)
plt.title('Distribution of Price')
plt.xlabel('Price')
plt.ylabel('Frequency')
plt.show()

"""Sebagian besar tempat wisata memiliki harga yang rendah atau gratis. Hal ini terlihat dari tingginya frekuensi pada rentang harga 0 hingga sekitar 25000.

### Rating
"""

gradient_blue = LinearSegmentedColormap.from_list("gradient_blue", ["#add8e6", "#00008b"])
plt.figure(figsize=(6, 4))
rating_counts = place_tourism['Rating'].value_counts().sort_values(ascending=True)

# Buat vertical bar plot dengan gradient blue color
bars = plt.bar(
    rating_counts.index,
    rating_counts.values,
    color=gradient_blue([i / len(rating_counts) for i in range(len(rating_counts))])
)

for bar in bars:
    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height(),
             str(int(bar.get_height())),
             va='bottom',
             ha='center',
             fontsize=9,
             color='black')

plt.suptitle('Distribusi Data Rating Tempat Wisata', fontsize=16)
plt.xlabel('Rating', fontsize=12)  # Label sumbu x
plt.ylabel('Jumlah', fontsize=12)  # Label sumbu y

# Rotasi label sumbu x agar mudah dibaca
plt.xticks(rotation=45, ha='right')

# Sesuaikan layout dan tampilkan
plt.tight_layout()
plt.show()

import seaborn as sns

sns.histplot(place_tourism['Rating'], kde=True)
plt.title('Distribution of Rating')
plt.xlabel('Rating')
plt.ylabel('Frequency')
plt.show()

sns.boxplot(x='Rating', data=place_tourism)
plt.title('Distribution of Rating')
plt.show()

"""Sebagian besar tempat wisata memiliki harga yang rendah atau gratis. Hal ini terlihat dari tingginya frekuensi pada rentang harga 0 hingga sekitar 25000.

## Univariate EDA on **Rating**
"""

rating

plt.figure(figsize=(8, 3))
sns.histplot(place_tourism['Rating'], kde=True, bins=20, color='skyblue', label='Histogram')
sns.kdeplot(place_tourism['Rating'], color='red', linewidth=2, label='KDE')
plt.title('Distribusi Rating Tempat Wisata dengan KDE', fontsize=10)
plt.xlabel('Rating', fontsize=9)
plt.ylabel('Frequency/Density', fontsize=9)
plt.legend()
plt.show()

"""Berdasarkan grafik distribusi rating tempat wisata dengan KDE, terlihat bahwa sebagian besar tempat wisata mendapatkan rating di rentang 4.0 hingga 4.5. Hal ini menunjukkan bahwa mayoritas tempat wisata di dataset tersebut memiliki kualitas yang baik dan memuaskan bagi pengunjung. Puncak distribusi KDE berada di sekitar rating 4.2, yang mengindikasikan bahwa rating tersebut merupakan rating yang paling umum diberikan oleh pengguna. Selain itu, distribusi rating cenderung condong ke kanan (right-skewed), yang berarti terdapat beberapa tempat wisata dengan rating yang sangat tinggi, meskipun jumlahnya relatif sedikit. Secara keseluruhan, distribusi rating tempat wisata menunjukkan bahwa mayoritas tempat wisata memiliki kualitas yang baik dan memuaskan bagi pengunjung, dengan beberapa tempat wisata yang memiliki rating sangat tinggi.

## Univariate EDA on **User**
"""

user

"""### Location
Analisis persebaran lokasi dengan visualisasi data bar plot untuk
"""

gradient_blue = LinearSegmentedColormap.from_list("gradient_blue", ["#add8e6", "#00008b"])
plt.figure(figsize=(9, 7))
location_counts = user['Location'].value_counts().sort_values(ascending=True)

# Buat horizontal bar plot dengan gradient blue color
bars = plt.barh(
    location_counts.index,
    location_counts.values,
    color=gradient_blue([i / len(location_counts) for i in range(len(location_counts))])
)

for bar in bars:
    plt.text(bar.get_width(), bar.get_y() + bar.get_height() / 2,
             str(int(bar.get_width())),
             va='center',
             ha='left',
             fontsize=9,
             color='black')

plt.suptitle('Distribusi Data Lokasi Wisata', fontsize=16)
plt.xlabel('Jumlah', fontsize=12)
#plt.ylabel('Lokasi', fontsize=12)

# Sesuaikan layout dan tampilkan
plt.tight_layout()
plt.show()

"""Berdasarkan visualisasi data, terlihat bahwa lokasi wisata terbanyak berada di Jawa Barat, diikuti oleh Semarang dan Yogyakarta. Selanjutnya, yang menjadi tempat lokasi wisata paling sedikit adalah Nganjuk dan Madura

### Age

Rentang usia pengunjung dengan menggunakan visualisasi data Plot Histogram, QQ-Plot, dan Box Plot
"""

# --- Variable, Color & Plot Size ---
var = 'Age'
fig = plt.figure(figsize=(12, 12))

# --- Skewness & Kurtosis ---
print('\033[1m'+'.: Age Column Skewness & Kurtosis :.'+'\033[0m')
print('*' * 40)
print('Skewness:'+'\033[1m {:.3f}'.format(user[var].skew(axis=0, skipna=True)))
print('\033[0m'+'Kurtosis:'+'\033[1m {:.3f}'.format(user[var].kurt(axis=0, skipna=True)))
print('\n')

# --- General Title ---
fig.suptitle('Distribusi Usia Pengunjung',
             fontweight='bold',
             fontsize=16,
             fontfamily='sans-serif')
fig.subplots_adjust(top=0.9)

# --- Histogram ---
ax_1 = fig.add_subplot(2, 2, 2)
plt.title('Histogram Plot',
          fontweight='bold',
          fontsize=14,
          fontfamily='sans-serif')

sns.histplot(data=user,
             x=var,
             kde=True)

plt.xlabel('Total',
           fontweight='regular',
           fontsize=11,
           fontfamily='sans-serif')

plt.ylabel('Age',
           fontweight='regular',
           fontsize=11,
           fontfamily='sans-serif')

# --- Q-Q Plot ---
ax_2 = fig.add_subplot(2, 2, 4)
plt.title('Q-Q Plot',
          fontweight='bold',
          fontsize=14,
          fontfamily='sans-serif')

sm.qqplot(user[var],
       fit=True,
       line='45',
       ax=ax_2,
       color='#BA1141')

plt.xlabel('Theoretical Quantiles',
           fontweight='regular',
           fontsize=11,
           fontfamily='sans-serif')

plt.ylabel('Sample Quantiles',
           fontweight='regular',
           fontsize=11,
           fontfamily='sans-serif')

# --- Box Plot ---
ax_3 = fig.add_subplot(1, 2, 1)
plt.title('Box Plot', fontweight='bold', fontsize=14, fontfamily='sans-serif')
sns.boxplot(data=user, y=var, boxprops=dict(alpha=0.8), linewidth=1.5)
plt.ylabel('Age', fontweight='regular', fontsize=11, fontfamily='sans-serif')

plt.show()

"""Berdasarkan analisis distribusi usia pengunjung, diketahui bahwa mayoritas pengunjung berusia antara 20 hingga 30 tahun. Distribusi data usia cenderung terdistribusi normal dengan sedikit kemiringan ke kanan (right-skewed), mengindikasikan adanya beberapa pengunjung dengan usia yang lebih tua. Meskipun terdapat outlier pada data usia, namun secara umum dapat disimpulkan bahwa tempat wisata tersebut lebih banyak dikunjungi oleh kalangan muda. Hal ini dapat menjadi pertimbangan dalam strategi pemasaran dan pengembangan produk wisata yang disesuaikan dengan preferensi dan kebutuhan pengunjung dari rentang usia tersebut.

# ⭐ Data Preparation : Content-Based Filtering⭐

## Feature Selection for Place_tourism

Pada tahap ini, dataset place_tourism dan rating akan digabungkan untuk pemodelan content-based filtering. sehingga hanya beberapa kolom saja yang akan digunakan dalam dataset
"""

place_tourism.drop(['Rating','Time_Minutes','Coordinate','Lat','Long'],axis=1,inplace=True)
place_tourism.head()

"""## Menggabungkan Data Place_Tourism dengan Rating

Penggabungan dataset dengan menggunakan function pd.merge() untuk melakukan join data. Pada hal ini unique key value yang dipakai adalah Place_Id
"""

merged_data = pd.merge(rating.groupby('Place_Id')['Place_Ratings'].mean(),
                       place_tourism,
                       on='Place_Id')
merged_data

"""Tahap selanjutnya adalah membuat copy data. Kode ini berfungsi untuk membuat salinan (copy) dari DataFrame merged_data dan menyimpannya ke dalam variabel baru bernama data_content_based."""

# Membuat Copy Data
data_content_based = merged_data.copy()
data_content_based

"""## Text Processing

Pertama, mendefinisikan sebuah fungsi bernama preprocessing yang bertujuan untuk melakukan pra-pemrosesan teks dalam bahasa Indonesia. Fungsi ini menggunakan library Sastrawi, sebuah library Python untuk pemrosesan bahasa alami bahasa Indonesia.

Pertama, fungsi preprocessing mengubah teks menjadi huruf kecil dengan menggunakan data.lower(). Kemudian, fungsi ini melakukan stemming, yaitu proses mengubah kata menjadi bentuk dasarnya, dengan menggunakan objek stem yang dibuat dari StemmerFactory. Setelah itu, fungsi ini menghapus stop words, yaitu kata-kata umum yang tidak memiliki makna penting dalam analisis teks, dengan menggunakan objek stopword yang dibuat dari StopWordRemoverFactory.

Hasil dari fungsi preprocessing adalah teks yang telah diubah menjadi huruf kecil, di-stemming, dan dihapus stop words-nya. Teks yang telah diproses ini kemudian dapat digunakan untuk analisis teks lebih lanjut, seperti pemodelan topik atau klasifikasi teks.


"""

stem = StemmerFactory().create_stemmer()
stopword = StopWordRemoverFactory().create_stop_word_remover()

def preprocessing(data):
    data = data.lower()
    data = stem.stem(data)
    data = stopword.remove(data)
    return data

"""
- Case Folding: data = data.lower(): Mengubah semua teks menjadi huruf kecil untuk penyeragaman.
- Stemming: data = stem.stem(data): Memotong imbuhan kata menjadi kata dasar menggunakan library Sastrawi. Contoh: "bermain" menjadi "main".
- Stop Word Removal: data = stopword.remove(data): Menghilangkan kata-kata umum (stop words) seperti "yang", "dan", "di", dll., yang dianggap tidak memiliki makna penting dalam analisis teks.
- Kode ini menerapkan fungsi preprocessing yang telah didefinisikan sebelumnya pada setiap baris data di kolom 'Pattern'.
- Hasilnya, data teks di kolom 'Pattern' telah dibersihkan dan diubah menjadi bentuk yang lebih sederhana dan terstruktur, siap untuk digunakan dalam pemodelan content-based filtering."""

# Menyatukan Deskripsi dan Kategori ke dalam kolom Pattern
data_content_based['Pattern'] = data_content_based['Description'] + ' ' + data_content_based['Category']
# Menghilangkan kolom Price, Place Rating, Deskripsi, dan City
data_content_based.drop(['Price','Place_Ratings','Description','City'],axis=1,inplace=True)

data_content_based

# Menerapkan Function
data_content_based['Pattern'] = data_content_based['Pattern'].apply(preprocessing)
data_content_based

"""# ⭐ Modeling Content-Based Filtering⭐

## TF-IDF

TF-IDF adalah teknik dalam Natural Language Processing (NLP) yang digunakan untuk merepresentasikan teks dalam bentuk numerik. Teknik ini bertujuan untuk menilai seberapa penting suatu kata dalam sebuah dokumen relatif terhadap kumpulan dokumen (corpus). TF-IDF sering digunakan dalam sistem rekomendasi berbasis konten dan tugas seperti pencarian teks.

TF-IDF terdiri dari dua komponen utama:

Term Frequency (TF): Mengukur seberapa sering sebuah kata muncul dalam dokumen tertentu.
Inverse Document Frequency (IDF): Mengukur pentingnya kata tersebut di seluruh dokumen dalam corpus.

- Sebelum membangun sistem rekomendasi berbasis content-based filtering, persiapkan data dan simpan dalam variabel baru bernama data.
- Selanjutnya, buat sistem rekomendasi berdasarkan tempat wisata yang pernah dikunjungi sebelumnya dengan memanfaatkan TF-IDF Vectorizer dari pustaka scikit-learn. Langkah ini meliputi inisialisasi TfidfVectorizer, perhitungan nilai idf pada kolom place_name, serta pemetaan indeks fitur ke nama fitur.
- Lakukan proses fitting dan transformasi pada fitur place_name untuk menghasilkan matriks representasi data.
Ubah vektor hasil transformasi TF-IDF menjadi matriks menggunakan fungsi todense()
"""

from sklearn.feature_extraction.text import TfidfVectorizer

# Inisialisasi TfidfVectorizer
tfidf = TfidfVectorizer()

# Melakukan perhitungan idf pada data cuisine
tfidf.fit(data_content_based['Pattern'])

# Mapping array dari fitur index integer ke fitur nama
tfidf.get_feature_names_out()

"""Selanjutnya, lakukan fit dan transformasi ke dalam bentuk matriks."""

# Melakukan fit lalu ditransformasikan ke bentuk matrix
tfidf_matrix = tfidf.fit_transform(data_content_based['Pattern'])

# Melihat ukuran matrix tfidf
tfidf_matrix.shape

"""Matriks yang dihasilkan berukuran (437, 4962). Nilai 437 merupakan ukuran data dan 4962 merupakan matrik Pattern.

Untuk menghasilkan vektor tf-idf dalam bentuk matriks, gunakan fungsi todense().
"""

# Mengubah vektor tf-idf dalam bentuk matriks dengan fungsi todense()
tfidf_matrix.todense()

"""**Membuat dataframe untuk melihat tf-idf Matrix**"""

# Membuat dataframe untuk melihat tf-idf matrix

pd.DataFrame(
    tfidf_matrix.todense(),
    columns=tfidf.get_feature_names_out(),
    index=data_content_based.Pattern
).sample(22, axis=1).sample(10, axis=0)

"""## Cosine Similarity

Cosine similarity digunakan untuk mengukur sejauh mana dua data tempat (place) memiliki kesamaan dengan menghitung sudut antara keduanya. Teknik ini menilai tingkat kesamaan berdasarkan sudut antara data tempat yang sedang dianalisis. Hasil perhitungan ini akan menghasilkan nilai yang mencerminkan tingkat kesamaan antara dua data tempat, di mana nilai yang mendekati 1 menunjukkan kesamaan yang tinggi, dan nilai yang mendekati 0 menunjukkan kesamaan yang rendah.

- Selanjutnya, buat sebuah dataframe yang menampilkan matriks TF-IDF dengan kolom berisi Place_Name dan baris Category. Dataframe ini digunakan untuk menganalisis hubungan antara place_name dan kategorinya.
- Setelah itu, hitung tingkat kesamaan (similarity degree) antar place_name dengan memanfaatkan fungsi cosine_similarity dari pustaka scikit-learn.
"""

cosine_sim = cosine_similarity(tfidf_matrix)
cosine_sim

"""Setelah mendapatkan matriks TF-IDF, kita menghitung cosine similarity untuk mengetahui seberapa mirip setiap tempat wisata satu sama lain. Perhitungan ini dilakukan menggunakan fungsi cosine_similarity dari library sklearn. Untuk visualisasi, matriks kesamaan ditampilkan sebagian dengan memilih 5 kolom dan 10 baris secara acak yang berisi nama-nama tempat wisata."""

# Membuat dataframe dari variabel cosine_sim dengan baris dan kolom berupa nama tempat wisata
cosine_sim_df = pd.DataFrame(cosine_sim,
                             index=data_content_based['Place_Name'],
                             columns=data_content_based['Place_Name'])

print('Shape:', cosine_sim_df.shape)

# Melihat similarity matrix pada setiap tempat wisata
cosine_sim_df.sample(5, axis=1).sample(10, axis=0)

"""Dengan cosine similarity, kita berhasil mengidentifikasi kesamaan antara satu restoran dengan restoran lainnya. Shape: (437, 437)merupakan ukuran matriks similarity dari data yang kita miliki. Berdasarkan data yang ada, matriks di atas sebenarnya berukuran 437 Nama Tempat Wisata x 437 Nama Tempat Wisata (masing-masing dalam sumbu X dan Y).

Artinya, kita mengidentifikasi tingkat kesamaan pada 437 Nama Tempat Wisata. Tapi tentu kita tidak bisa menampilkan semuanya. Oleh karena itu, kita hanya memilih 10 Nama Tempat Wisata pada baris vertikal dan 5 restoran pada sumbu horizontal

## Develop Content-Based Filtering Recommender System

Selanjutnya, dibuatkan function untuk mendapatkan rekomendasi berdasarkan kode berikut
"""

def recommend_by_content_based_filtering(nama_tempat, similarity_data=cosine_sim_df, items_data=data_content_based[['Place_Name','Pattern','Category']], k=10):
    """
    Recommends items using content-based filtering and returns a DataFrame.

    Args:
        nama_tempat (str): The name of the item to find recommendations for.
        similarity_data (DataFrame): The DataFrame containing the similarity matrix.
        items_data (DataFrame): The DataFrame containing the item data.
        k (int, optional): The number of recommendations to generate. Defaults to 10.

    Returns:
        DataFrame: A DataFrame containing the recommendations, or an empty DataFrame if the item is not found.
    """

    # Find the index of the given item
    filtered_items = items_data[items_data['Place_Name'] == nama_tempat]

    # Check if the item exists in the dataset
    if filtered_items.empty:
        print(f"Item '{nama_tempat}' not found in the dataset.")
        return pd.DataFrame()  # Return an empty DataFrame if item not found

    nama_tempat_index = filtered_items.index[0]

    # Get the similarity scores for the given item
    similarity_scores = similarity_data.iloc[nama_tempat_index]

    # Sort the items by similarity score in descending order
    nama_tempat_list = similarity_scores.sort_values(ascending=False).index[1:k + 1]

    # Create a DataFrame containing the recommendations
    recommended_items = items_data[items_data['Place_Name'].isin(nama_tempat_list)][['Place_Name', 'Category']]
    recommended_items['similarity_score'] = similarity_scores[nama_tempat_list].values

    return recommended_items

"""Parameter Fungsi

1. **`nama_tempat`**  
   Nama tempat wisata yang akan dicari rekomendasinya (dalam bentuk *string*).  
   
2. **`similarity_data`**  
   DataFrame berisi matriks kesamaan (*similarity matrix*) antar tempat wisata (default: `cosine_sim_df`).  

3. **`items_data`**  
   DataFrame yang memuat informasi tempat wisata, termasuk kolom *Place_Name*, *Pattern*, dan *Category*.  

4. **`k`**  
   Jumlah rekomendasi yang ingin dihasilkan (default: 10).

## Get Recommendation
"""

# Skenario 1 : Candi Prambanan
recommend_by_content_based_filtering('Candi Prambanan')

"""Pada Skenario 1 dengan inputan **Candi Prambanan** yang merupakan kategori **Budaya** diperoleh bahwa dari 10 data yang direkomendasikan, model berhasil merekomendasikan 9 data categori yang sama"""

# Case 2 : Museum Basoeki Abdullah
recommend_by_content_based_filtering('Museum Basoeki Abdullah')

"""Pada Skenario 2 dengan inputan **Museum Basoeki Abdullah** yang merupakan kategori **Budaya** diperoleh bahwa dari 10 data yang direkomendasikan, model berhasil merekomendasikan 10 data categori yang sama"""

recommend_by_content_based_filtering('Dunia Fantasi')

"""Pada Skenario 3 dengan inputan **Dunia Fantasi** yang merupakan kategori **Taman Hiburan** diperoleh bahwa dari 8 data yang direkomendasikan, model berhasil merekomendasikan 10 data categori yang sama

# ⭐ Modeling Collaborative Filtering⭐

## Data Understanding

Data yang digunakan pada Model Collaborative Filtering adalah data **rating**
"""

data_collaborative_filtering = rating.copy()
data_collaborative_filtering

"""## Data Preparation

- Pada tahapan mengubah User_Id menjadi list unik, kode mengambil kolom User_Id dari data data_collaborative_filtering, lalu mengekstrak nilai unik dari kolom tersebut menggunakan .unique(). Hasilnya diubah menjadi list menggunakan .tolist() yang bertujuan untuk menghilangkan duplikasi nilai User_Id, sehingga hanya nilai unik yang digunakan. Hal ini penting untuk memastikan setiap pengguna di-encode hanya sekali.

- Selanjutnya, Membuat dictionary (user_to_user_encoded) yang memetakan setiap User_Id unik ke indeks angka menggunakan fungsi enumerate dengan tujuan untuk mengubah User_Id menjadi representasi numerik, yang lebih efisien untuk diproses oleh algoritma machine learning, terutama dalam operasi matriks seperti collaborative filtering.

- Terakhir, Membuat dictionary (user_encoded_to_user) yang merupakan kebalikan dari user_to_user_encoded. Dictionary ini memetakan indeks angka kembali ke nilai User_Id. Hal ini memungkinkan decoding hasil prediksi (yang dalam format numerik) kembali ke format asli User_Id agar lebih mudah dipahami atau
ditampilkan.
"""

# Mengubah userID menjadi list tanpa nilai yang sama
user_ids = data_collaborative_filtering['User_Id'].unique().tolist()
print('list userID: ', user_ids)

# Melakukan encoding userID
user_to_user_encoded = {x: i for i, x in enumerate(user_ids)}
print('encoded userID : ', user_to_user_encoded)

# Melakukan proses encoding angka ke ke userID
user_encoded_to_user = {i: x for i, x in enumerate(user_ids)}
print('encoded angka ke userID: ', user_encoded_to_user)

"""Hasil yang diperoleh :


Sehingga hasil pada tahapan ini adalah sebagai berikut

| userID | Encoded userID | Decoded userID |
|--------|----------------|----------------|
| 1      | 0              | 1              |
| 2      | 1              | 2              |
| 3      | 2              | 3              |
| 4      | 3              | 4              |
| 5      | 4              | 5              |
| 6      | 5              | 6              |
| 7      | 6              | 7              |
| 8      | 7              | 8              |
| 9      | 8              | 9              |
| 10     | 9              | 10             |
| ...    | ...            | ...            |
| 300    | 299            | 300            |
"""

# Mengubah placeID menjadi list tanpa nilai yang sama
place_ids = data_collaborative_filtering['Place_Id'].unique().tolist()

# Melakukan proses encoding placeID
place_to_place_encoded = {x: i for i, x in enumerate(place_ids)}

# Melakukan proses encoding angka ke placeID
place_encoded_to_place = {i: x for i, x in enumerate(place_ids)}

"""Pertama, kode ini mengambil nilai unik dari kolom Place_Id dalam dataframe data_collaborative_filtering dan mengubahnya menjadi sebuah list tanpa nilai yang sama. Selanjutnya, dilakukan encoding terhadap Place_Id dengan membuat dua dictionary: place_to_place_encoded, yang memetakan setiap Place_Id ke indeks numerik yang unik, dan place_encoded_to_place, yang melakukan pemetaan terbalik dari indeks ke Place_Id.

Kemudian, kode ini memetakan User_Id ke dataframe dengan menggunakan dictionary user_to_user_encoded, sehingga setiap pengguna mendapatkan representasi numerik. Selanjutnya, Place_Id juga dipetakan ke dataframe menggunakan dictionary place_to_place_encoded, menghasilkan kolom baru bernama place_encoded.
"""

# Mapping userID ke dataframe user
data_collaborative_filtering['user'] = data_collaborative_filtering['User_Id'].map(user_to_user_encoded)

# Mapping placeID ke dataframe
data_collaborative_filtering['place_encoded'] = data_collaborative_filtering['Place_Id'].map(place_to_place_encoded)

"""
Setelah proses pemetaan, kode menghitung jumlah pengguna dan jumlah tempat yang ada dalam data dengan menghitung panjang dari dictionary yang telah dibuat. Selain itu, rating tempat diubah menjadi tipe data float untuk memudahkan analisis lebih lanjut. Kode ini juga menentukan nilai minimum dan maksimum dari rating yang diberikan. Terakhir, informasi mengenai jumlah pengguna, jumlah tempat, serta nilai minimum dan maksimum rating dicetak ke layar dalam format yang jelas. Kode ini secara keseluruhan bertujuan untuk mempersiapkan data agar siap digunakan dalam algoritma rekomendasi."""

# Mendapatkan jumlah user
num_users = len(user_to_user_encoded)
print(num_users)

# Mendapatkan jumlah resto
num_place = len(place_to_place_encoded)
print(num_place)

# Mengubah rating menjadi nilai float
data_collaborative_filtering['Place_Ratings'] = data_collaborative_filtering['Place_Ratings'].values.astype(np.float32)

# Nilai minimum rating
min_rating = min(data_collaborative_filtering['Place_Ratings'])

# Nilai maksimal rating
max_rating = max(data_collaborative_filtering['Place_Ratings'])

print('Number of User: {}, Number of Places: {}, Min Rating: {}, Max Rating: {}'.format(
    num_users, num_place, min_rating, max_rating
))

"""## Spliting data

Dua kolom dari dataframe data_collaborative_filtering diambil, yaitu user dan place_encoded, dan mengubahnya menjadi array numpy yang disimpan dalam variabel x. Array ini akan digunakan sebagai fitur input untuk model, di mana setiap baris mewakili pasangan pengguna dan tempat.

Selanjutnya, kode membuat variabel y, yang berisi rating tempat yang telah dinormalisasi. Proses normalisasi dilakukan dengan menggunakan rumus min-max scaling, di mana setiap rating dikurangi dengan nilai minimum (min_rating) dan dibagi dengan rentang rating (selisih antara nilai maksimum dan minimum). Hasilnya adalah rating yang berada dalam rentang 0 hingga 1, yang lebih mudah digunakan dalam model pembelajaran mesin.
"""

data_collaborative_filtering = data_collaborative_filtering.sample(frac=1, random_state=42)
data_collaborative_filtering

"""Setelah itu, **data dibagi menjadi dua set: 80% untuk data pelatihan (train) dan 20% untuk data validasi (validation)**. Indeks pemisahan ditentukan dengan menghitung 80% dari jumlah total baris dalam dataframe. Kemudian, x_train dan y_train berisi data pelatihan, sedangkan x_val dan y_val berisi data validasi.

Terakhir, kode mencetak nilai dari x dan y, memberikan gambaran tentang data yang telah dipersiapkan untuk digunakan dalam pelatihan model rekomendasi. Proses ini penting untuk memastikan bahwa model dapat belajar dari data yang representatif dan juga dapat diuji pada data yang tidak terlihat sebelumnya untuk mengevaluasi performanya.

"""

x = data_collaborative_filtering[['user', 'place_encoded']].values

# Membuat variabel y untuk membuat rating dari hasil
y = data_collaborative_filtering['Place_Ratings'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values

# Membagi menjadi 80% data train dan 20% data validasi
train_indices = int(0.8* data_collaborative_filtering.shape[0])
x_train, x_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:]
)

print(x, y)

"""## Model Development and Training

### RecommenderNet


**1. Inisialisasi Model**

**Konstruktor (__init__)**

Parameter:
- num_users: Jumlah pengguna dalam dataset.
- num_place: Jumlah tempat (restoran) dalam dataset.
- embedding_size: Ukuran vektor embedding untuk pengguna dan tempat.

Layer Embedding:
- user_embedding: Layer ini digunakan untuk mengubah ID pengguna menjadi vektor embedding berukuran embedding_size. Vektor ini diinisialisasi menggunakan distribusi normal He (he_normal) dan dilengkapi dengan regularisasi L2 untuk mencegah overfitting.
- user_bias: Layer ini menyimpan bias untuk setiap pengguna, yang merupakan nilai tambahan yang ditambahkan ke prediksi akhir untuk memperbaiki akurasi model.
- place_embedding: Serupa dengan user_embedding, layer ini mengubah ID tempat menjadi vektor embedding berukuran embedding_size, juga dengan inisialisasi dan regularisasi yang sama.
- place_bias: Layer ini menyimpan bias untuk setiap tempat, memberikan penyesuaian tambahan pada prediksi berdasarkan karakteristik spesifik dari tempat tersebut.

**2. Metode call**
Metode call adalah inti dari model, di mana proses komputasi dilakukan saat model dipanggil dengan input.
Input:
- inputs: Sebuah tensor dua dimensi yang berisi pasangan ID pengguna dan ID tempat. Setiap baris mewakili satu interaksi antara pengguna dan tempat.
- Proses:
- Mendapatkan Vektor Embedding:
- user_vector: Mengambil embedding pengguna berdasarkan ID pengguna dari kolom pertama input.
- user_bias: Mengambil bias pengguna berdasarkan ID pengguna.
- place_vector: Mengambil embedding tempat berdasarkan ID tempat dari kolom kedua input.
- place_bias: Mengambil bias tempat berdasarkan ID tempat.
- Menghitung Prediksi:
- dot_user_place: Menghitung produk titik (dot product) antara vektor embedding pengguna dan vektor embedding tempat. Ini memberikan ukuran seberapa cocok pengguna tersebut dengan tempat tertentu.
- x: Menjumlahkan hasil produk titik dengan bias pengguna dan bias tempat. Ini menghasilkan nilai prediksi yang belum dinormalisasi.

**Aktivasi:**
Model menggunakan fungsi aktivasi sigmoid (tf.nn.sigmoid(x)) pada output akhir. Fungsi sigmoid mengubah nilai prediksi menjadi rentang antara 0 dan 1, yang cocok untuk masalah regresi biner atau probabilitas rating.
"""

class RecommenderNet(tf.keras.Model):

  # Insialisasi fungsi
  def __init__(self, num_users, num_place, embedding_size, **kwargs):
    super(RecommenderNet, self).__init__(**kwargs)
    self.num_users = num_users
    self.num_place = num_place
    self.embedding_size = embedding_size
    self.user_embedding = layers.Embedding( # layer embedding user
        num_users,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.user_bias = layers.Embedding(num_users, 1) # layer embedding user bias
    self.place_embedding = layers.Embedding( # layer embeddings resto
        num_place,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.place_bias = layers.Embedding(num_place, 1) # layer embedding resto bias

  def call(self, inputs):
    user_vector = self.user_embedding(inputs[:,0]) # memanggil layer embedding 1
    user_bias = self.user_bias(inputs[:, 0]) # memanggil layer embedding 2
    place_vector = self.place_embedding(inputs[:, 1]) # memanggil layer embedding 3
    place_bias = self.place_bias(inputs[:, 1]) # memanggil layer embedding 4

    dot_user_place = tf.tensordot(user_vector, place_vector, 2)

    x = dot_user_place + user_bias + place_bias

    return tf.nn.sigmoid(x) # activation sigmoid

"""### Menggunakan Model RecommenderNet

1. Inisiasi Model
Parameter-parameter pada penggunaan model RecommenderNet adalah sebagai berikut

- RecommenderNet: Ini adalah kelas khusus yang telah didefinisikan sebelumnya untuk membangun model rekomendasi.

- num_users: Jumlah total pengguna dalam dataset.

- num_place: Jumlah total tempat (dalam konteks ini, mungkin tempat wisata) dalam dataset.

- 50: Menunjukkan ukuran embedding yang digunakan. Embedding adalah representasi vektor dari pengguna dan tempat, dan angka 50 menunjukkan dimensi dari vektor tersebut.

2. Kompilasi Model
- loss: Menentukan fungsi kerugian (loss function) yang digunakan untuk mengukur seberapa baik model memprediksi rating. Di sini, digunakan MeanSquaredError yang menghitung rata-rata kuadrat selisih antara prediksi dan nilai sebenarnya.

- optimizer: Menentukan algoritma optimasi yang digunakan untuk memperbarui parameter model selama pelatihan. RMSprop adalah salah satu algoritma optimasi yang umum digunakan. learning_rate mengatur seberapa besar langkah yang diambil oleh optimizer dalam memperbarui parameter.

- metrics: Menentukan metrik yang digunakan untuk mengevaluasi kinerja model. RootMeanSquaredError adalah akar kuadrat dari MeanSquaredError, dan memberikan ukuran kesalahan prediksi dalam skala yang sama dengan rating.

"""

model = RecommenderNet(num_users, num_place, 50) # inisialisasi model

# model compile
model.compile(
    loss = tf.keras.losses.MeanSquaredError(),
    optimizer = keras.optimizers.RMSprop(learning_rate=0.0001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

# Memulai training


early_stopping = tf.keras.callbacks.EarlyStopping(
    monitor='val_loss',  # Monitor validation loss
    patience=5,          # Number of epochs with no improvement after which training will be stopped
    restore_best_weights=True # Restore model weights from the epoch with the best value of the monitored quantity
)

history = model.fit(
    x = x_train,
    y = y_train,
    batch_size =32,
    epochs = 50,
    validation_data = (x_val, y_val),
)

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

"""**Interpretasi Hasil Model `RecommenderNet`**

1. Nilai Loss dan RMSE
Loss (0.1156): Nilai loss yang rendah menunjukkan bahwa model memiliki kesalahan yang kecil dalam memprediksi output pada data pelatihan.
RMSE (0.3398): RMSE memberikan gambaran tentang seberapa besar rata-rata deviasi prediksi model dari nilai sebenarnya. Nilai RMSE yang rendah menunjukkan bahwa prediksi model cukup akurat. Namun, interpretasi nilai ini juga bergantung pada skala dan konteks data yang digunakan. Misalnya, jika output yang diprediksi berada dalam rentang 0 hingga 1, maka RMSE sebesar 0.3398 mungkin dianggap cukup baik.

2. Validation Metrics
Val Loss (0.1256): Nilai loss pada data validasi sedikit lebih tinggi dibandingkan dengan loss pada data pelatihan, yang bisa mengindikasikan adanya overfitting. Ini berarti model mungkin telah belajar pola dari data pelatihan tetapi tidak dapat menggeneralisasi dengan baik ke data baru.
Val RMSE (0.3542): RMSE pada data validasi juga lebih tinggi dibandingkan dengan RMSE pada data pelatihan. Ini menunjukkan bahwa prediksi model pada data validasi kurang akurat dibandingkan dengan prediksi pada data pelatihan.

### RecommenderNet v2


**1. Inisialisasi Model**

**Konstruktor (`__init__`)**

- **Parameter**:
  - `num_users`: Jumlah pengguna dalam dataset.
  - `num_place`: Jumlah tempat (restoran) dalam dataset.
  - `embedding_size`: Ukuran vektor embedding untuk pengguna dan tempat.

- **Layer Embedding**:
  - **`user_embedding`**: Layer ini mengubah ID pengguna menjadi vektor embedding berukuran `embedding_size`. Vektor diinisialisasi menggunakan distribusi normal He (`he_normal`) dan dilengkapi dengan regularisasi L2 untuk mencegah overfitting.
  
  - **`user_bias`**: Layer ini menyimpan bias untuk setiap pengguna, memberikan penyesuaian tambahan pada prediksi akhir.

  - **`place_embedding`**: Layer ini mengubah ID tempat menjadi vektor embedding berukuran `embedding_size`, juga dengan inisialisasi dan regularisasi yang sama.

  - **`place_bias`**: Layer ini menyimpan bias untuk setiap tempat, memberikan penyesuaian tambahan pada prediksi berdasarkan karakteristik spesifik tempat tersebut.

**2. Metode `call`**

Metode `call` adalah bagian utama dari model, di mana proses komputasi dilakukan saat model dipanggil dengan input.

- **Input**:
  - `inputs`: Sebuah tensor dua dimensi yang berisi pasangan ID pengguna dan ID tempat. Setiap baris mewakili satu interaksi antara pengguna dan tempat.

- **Proses**:
  - **Mendapatkan Vektor Embedding**:
    - `user_vector`: Mengambil embedding pengguna berdasarkan ID pengguna dari kolom pertama input.
    - `user_bias`: Mengambil bias pengguna berdasarkan ID pengguna.
    - `place_vector`: Mengambil embedding tempat berdasarkan ID tempat dari kolom kedua input.
    - `place_bias`: Mengambil bias tempat berdasarkan ID tempat.
  
  - **Menghitung Prediksi**:
    - `dot_user_place`: Menghitung produk titik (dot product) antara vektor embedding pengguna dan vektor embedding tempat, memberikan ukuran seberapa cocok pengguna tersebut dengan tempat tertentu.
    
    - `x`: Menjumlahkan hasil produk titik dengan bias pengguna dan bias tempat. Ini menghasilkan nilai prediksi yang belum dinormalisasi.

- **Aktivasi**:
  - Model menggunakan fungsi aktivasi sigmoid (`tf.nn.sigmoid(x)`) pada output akhir. Fungsi sigmoid mengubah nilai prediksi menjadi rentang antara 0 dan 1, yang cocok untuk masalah regresi biner atau probabilitas rating.
"""

class RecommenderNetV2(tf.keras.Model):
    def __init__(self, num_users, num_place, embedding_size, **kwargs):
        super(RecommenderNet, self).__init__(**kwargs)
        self.num_users = num_users
        self.num_place = num_place
        self.embedding_size = embedding_size

        self.user_embedding = layers.Embedding(
            num_users,
            embedding_size,
            embeddings_initializer='he_normal',
            embeddings_regularizer=keras.regularizers.l2(1e-6)
        )
        self.user_bias = layers.Embedding(num_users, 1)
        self.place_embedding = layers.Embedding(
            num_place,
            embedding_size,
            embeddings_initializer='he_normal',
            embeddings_regularizer=keras.regularizers.l2(1e-6)
        )
        self.place_bias = layers.Embedding(num_place, 1)

    def call(self, inputs):
        user_vector = self.user_embedding(inputs[:, 0])
        user_bias = self.user_bias(inputs[:, 0])
        place_vector = self.place_embedding(inputs[:, 1])
        place_bias = self.place_bias(inputs[:, 1])

        dot_user_place = tf.tensordot(user_vector, place_vector, 2)

        # Combine the results
        x = dot_user_place + user_bias + place_bias
        x = tf.nn.sigmoid(x)  # Use sigmoid if your target is between 0 and 1

        # Add additional dense layers for more complexity
        x = layers.Dense(64, activation='relu')(x)
        x = layers.Dropout(0.2)(x)  # Dropout for regularization
        x = layers.Dense(32, activation='relu')(x)

        return x  # Return the final output (no activation for regression)

"""### Menggunakan Model RecommenderNetv2

- num_users dan num_place: Mewakili jumlah pengguna dan tempat unik, yang menentukan ukuran input model.
- 50: Dimensi embedding (faktor laten). Angka ini menentukan seberapa rinci representasi pengguna dan tempat. Nilai 50 memberikan keseimbangan antara kompleksitas dan efisiensi komputasi.

Pada paremeter loss, saya menggunakan `CategoricalCrossentropy` karena model menangani masalah klasifikasi multi-kelas, di mana setiap prediksi hanya termasuk dalam satu kategori.

Selanjutnya, pada Optimizer, beberapa paramater yang digunakan adalah

- Adamax: Variasi dari optimizer Adam yang stabil untuk dataset dengan distribusi yang jarang (sparse), seperti sistem rekomendasi.
- learning_rate=0.0001: Nilai learning rate yang kecil memungkinkan pembaruan parameter secara perlahan, mengurangi risiko melampaui solusi optimal.

Kemudian,
RootMeanSquaredError (RMSE) dipilih untuk mengevaluasi tingkat kesalahan prediksi pada sistem rekomendasi. RMSE adalah metrik yang umum digunakan untuk menilai seberapa dekat skor prediksi dengan skor aktual.
"""

# Initialize and compile the model
model = RecommenderNetv2(num_users, num_place, 50)

model.compile(
    #loss = tf.keras.losses.CategoricalFocalCrossentropy(),
    loss = tf.keras.losses.CategoricalCrossentropy(),
    optimizer = keras.optimizers.Adamax(learning_rate=0.0001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

# Train the model
early_stopping = keras.callbacks.EarlyStopping(monitor='val_root_mean_squared_error', patience=5)

history = model.fit(
    x=x_train,
    y=y_train,
    batch_size=64,
    epochs=50,
    validation_data=(x_val, y_val),
    callbacks=[early_stopping]
)

# Visualize the training and validation results
plt.plot(history.history['root_mean_squared_error'], label='Train RMSE')
plt.plot(history.history['val_root_mean_squared_error'], label='Validation RMSE')
plt.title('Training and Validation RMSE RecommendNetV2')
plt.xlabel('Epochs')
plt.ylabel('Root Mean Squared Error')
plt.legend()
plt.show()

"""1. Loss dan RMSE
Loss (6.1803e-08): Nilai loss yang sangat rendah ini menunjukkan bahwa model memiliki kesalahan yang sangat kecil dalam memprediksi output pada data pelatihan. Notasi 6.1803e-08 berarti 0.000000061803, yang menunjukkan performa model yang sangat baik.
Root Mean Squared Error (RMSE) (0.3475): RMSE ini juga menunjukkan bahwa rata-rata deviasi prediksi model dari nilai sebenarnya cukup kecil, meskipun nilainya sedikit lebih tinggi dibandingkan dengan loss.

2. Validation Metrics
Val Loss (6.0744e-08): Nilai loss pada data validasi juga sangat rendah, hanya sedikit lebih rendah dari loss pada data pelatihan, yang menunjukkan bahwa model tidak hanya bekerja baik pada data pelatihan tetapi juga dapat generalisasi dengan baik ke data baru.
Val RMSE (0.3472): RMSE pada data validasi hampir sama dengan RMSE pada data pelatihan, menunjukkan bahwa model mampu mempertahankan akurasi prediksi yang baik di kedua set data.

## Mendapatkan Rekomendasi

Membuat salinan dari dataset place_tourism ke dalam variabel data_places.
"""

data_places = place_tourism.copy()

df = rating

# Mengambil sample user
user_id = df.User_Id.sample(1).iloc[0]
places_visited_by_user = df[df.User_Id == user_id]

"""Mengambil satu User _Id secara acak dari dataset rating dan menyimpan tempat-tempat yang telah dikunjungi oleh pengguna tersebut ke dalam places_visited_by_user dan Mengubah placeID Menjadi List Tanpa Nilai yang Sama"""

# Mengubah placeID menjadi list tanpa nilai yang sama
place_ids = data_collaborative_filtering['Place_Id'].unique().tolist()

# Melakukan proses encoding placeID
place_to_place_encoded = {x: i for i, x in enumerate(place_ids)}

# Melakukan proses encoding angka ke placeID
place_encoded_to_place = {i: x for i, x in enumerate(place_ids)}

"""Mengambil Place_Id dari tempat-tempat yang belum dikunjungi oleh pengguna dengan membandingkan dengan daftar tempat yang telah dikunjungi. Kemudian, mengubah Place_Id yang belum dikunjungi menjadi format yang sesuai untuk model."""

places_not_visited = data_places[~data_places['Place_Id'].isin(places_visited_by_user.Place_Id.values)]['Place_Id']

place_not_visited = list(set(places_not_visited).intersection(set(place_to_place_encoded.keys())))

places_not_visited = [[place_to_place_encoded.get(x)] for x in places_not_visited]

user_encoder = user_to_user_encoded.get(user_id)

user_place_array = np.hstack(([[user_encoder]] * len(places_not_visited), places_not_visited))

"""Membuat array yang berisi pasangan pengguna dan tempat yang belum dikunjungi, lalu melakukan prediksi rating menggunakan model. Hasil prediksi disimpan dalam variabel ratings."""

ratings = model.predict(user_place_array).flatten()

"""Mengambil 10 rekomendasi tempat dengan rating tertinggi berdasarkan hasil prediksi dan menampilkan nama-nama tempat tersebut."""

print('Showing recommendations for users: {}'.format(user_id))
print('===' * 9)
print('Places with high ratings from user')
print('----' * 8)

# Sort places visited by user by Place_Ratings in descending order
places_visited_by_user = places_visited_by_user.sort_values(by='Place_Ratings', ascending=False)

# Get top 5 place IDs from sorted data
top_places_user = places_visited_by_user.head(5).Place_Id.values

# Filter place_tourism (renamed to data_places) by place IDs
data_places_rows = data_places[data_places['Place_Id'].isin(top_places_user)]

# Iterate through filtered data and print place names
for row in data_places_rows.itertuples():
    print(row.Place_Name)

print('----' * 8)
print('Top 10 place recommendation')
print('----' * 8)

# Get the indices of the top 10 recommendations from the ratings array
top_ratings_indices = ratings.argsort()[-10:][::-1]

# Get the corresponding place IDs from place_encoded_to_place
recommended_place_ids = [place_encoded_to_place.get(i) for i in top_ratings_indices] # Define recommended_place_ids

# Filter data_places by recommended_place_ids
recommended_places = data_places[data_places['Place_Id'].isin(recommended_place_ids)]

# Iterate through filtered data and print place names
for row in recommended_places.itertuples():
    print(row.Place_Name)

"""Untuk pengguna 173, beberapa tempat dengan rating tinggi yang disukai mencakup Taman Hutan Tebet, Museum Sonobudoyo Unit I, Pesona Nirwana Waterpark & Cottages, Kota Mini, dan Food Junction Grand Pakuwon. Hal ini menunjukkan bahwa pengguna tersebut memiliki ketertarikan yang kuat terhadap lokasi yang menawarkan pengalaman rekreasi dan budaya. Selain itu, model juga memberikan daftar sepuluh rekomendasi tempat teratas yang beragam, termasuk Taman Pintar Yogyakarta, Taman Pelangi Yogyakarta, Bentara Budaya Yogyakarta (BBY), Pantai Patihan, Pantai Greweng, Kampung Korea Bandung, Sanghyang Heuleut, Chingu Cafe Little Seoul, Wisata Lereng Kelir, dan Hutan Bambu Keputih. Rekomendasi ini mencakup berbagai jenis tempat, mulai dari taman dan pantai hingga kafe dan lokasi budaya, menunjukkan keberhasilan model dalam mengidentifikasi tempat populer di kalangan pengguna lain dengan preferensi serupa"""