# -*- coding: utf-8 -*-
"""Indonesian Tourism Recommender System

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aDYc39DCoRTAG-UJqXb9KHDef-_bUgzE
"""

!pip install sastrawi
!pip install kaggle
!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

!rm -rf idonesian_tourism/
!kaggle datasets download -d aprabowo/indonesia-tourism-destination -p /content/dataset --unzip

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from matplotlib.colors import LinearSegmentedColormap
import re
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import Pipeline

from Sastrawi.Stemmer.StemmerFactory import StemmerFactory
from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory
from sklearn.metrics.pairwise import cosine_similarity
from io import BytesIO
import base64
import statsmodels.api as sm # Add this import to your existing imports

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras import layers
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense

"""# ⭐ Data Understanding ⭐

# Place Tourism

| Nama Kolom | Tipe Data | Deskripsi |
|---|---|---|
| Place_Id | int64 | ID unik untuk setiap tempat |
| Place_Name | object | Nama tempat wisata |
| Description | object | Deskripsi tempat |
| Category | object | Kategori tempat (misalnya, Alam, Budaya) |
| City | object | Kota tempat wisata berada |
| Price | float64 | Harga atau biaya masuk |
| Rating | float64 | Rating rata-rata tempat |
| Time_Minutes | float64 | Estimasi waktu kunjungan |
| Coordinate | object | Koordinat latitude dan longitude |
| Lat | float64 | Koordinat latitude |
| Long | float64 | Koordinat longitude |

### Reading Data
"""

place_tourism = pd.read_csv('/content/dataset/tourism_with_id.csv')
place_tourism.head()

"""### Assesing Data"""

print("Bentuk Data :", place_tourism.shape)
print(10*"=")
# Data information
print("Informasi pada setiap Kolom pada dataset place_toursim :")
place_tourism.info()
print(10*"=")
# Data Duplicated
print("Jumlah Duplikat Data :")
place_tourism.duplicated().sum()
# Data Null
print("Jumlah Null Data :")
place_tourism.isna().sum()

print("Jumlah Unique Data :")
place_tourism.nunique()

"""Dari hasil assessing data pada dataset **`place_tourism`** diperoleh bahwa :

- Tidak ada data duplikasi pada dataset
- Terdapat missing value pada kolom Time Minutes sebanyak 232 dan Unnamed :11 sebanyak 437. sehingga perlu dilakukan cleaning

# Rating

### Reading Data
"""

rating = pd.read_csv('/content/dataset/tourism_rating.csv')
rating.head(3)

"""### Assesing Data"""

print("Bentuk Data :", rating.shape)
print(10*"=")
# Data information
print("Informasi pada setiap Kolom pada dataset place_toursim :")
rating.info()
print(10*"=")
# Data Duplicated
print("Jumlah Duplikat Data :")
rating.duplicated().sum()
# Data Null
print("Jumlah Null Data :")
rating.isna().sum()

print("Jumlah Unique Data :")
rating.nunique()

"""Pada dataset Rating, diketahui bahwa raw data sebanyak 10000 dan memiliki 3 variabel `User_id`, `place_id` dan `Place_Ratings` tidak memiliki missing value dan  tidak memiliki data yang duplikat

# User

### Reading Data
"""

user = pd.read_csv('/content/dataset/user.csv')
user.head(3)

"""### Assesing Data"""

print("Bentuk Data :", user.shape)
print(10*"=")
# Data information
print("Informasi pada setiap Kolom pada dataset place_toursim :")
user.info()
print(10*"=")
# Data Duplicated
print("Jumlah Duplikat Data :")
user.duplicated().sum()
# Data Null
print("Jumlah Null Data :")
user.isna().sum()

"""Dataset user memiliki 300 baris dan 3 kolom. Dataset ini tidak memiliki duplicated data dan tidak memiliki missing value

# ⭐ Data Preprocessing ⭐

## Handling Missing Value in Dataset Place Tourism
"""

place_tourism.isnull().sum()

# Imputation to Time_Minutes column
place_tourism['Time_Minutes'] = place_tourism['Time_Minutes'].fillna(place_tourism['Time_Minutes'].mean())

print("Setelah Dilakukan Filling Missing Value")
place_tourism.isnull().sum()

place_tourism.head(4)

"""## Menghilangkan Kolom yang Tidak Diinginkan"""

place_tourism.drop(['Unnamed: 11', 'Unnamed: 12'], axis=1, inplace=True)

place_tourism.head()

# Save Cleaned Data of place_tourism
place_tourism.to_csv('place_tourism_clean.csv', index=False)

"""# ⭐ Exploratory Data Analysis ⭐

## Univariate EDA on **place_tourism** dataset
"""

place_tourism

"""### Place Name
Ada berapa lokasi wisata dalam dataset?
"""

print("Total lokasi wisata :",place_tourism['Place_Name'].nunique())

"""### Category
Ada berapa lokasi wisata dalam dataset?
"""

gradient_blue = LinearSegmentedColormap.from_list("gradient_blue", ["#add8e6", "#00008b"])
plt.figure(figsize=(8, 3))
category_counts = place_tourism['Category'].value_counts().sort_values(ascending=True)

# Buat horizontal bar plot dengan gradient blue color
bars = plt.barh(
    category_counts.index,
    category_counts.values,
    color=gradient_blue([i / len(category_counts) for i in range(len(category_counts))])
)

for bar in bars:
    plt.text(bar.get_width(), bar.get_y() + bar.get_height() / 2,
             str(int(bar.get_width())),
             va='center',
             ha='left',
             fontsize=9,
             color='black')

plt.suptitle('Distribusi Data Kategori Tempat Wisata', fontsize=16)
plt.xlabel('Jumlah', fontsize=12)
#plt.ylabel('Kategori', fontsize=12)

# Sesuaikan layout dan tampilkan
plt.tight_layout()
plt.show()

"""Pada distribusi Data Kategori tempat Wisata, **Tempat Hiburan** menempati posisi teratassebanyak 135 kemudian disusul oleh **Budaya** sebesar 117. Adapun distribusi pada Kategori **Pusat Perbelanjaan** adalah sebanyak 15 yang menempati posisi terakhir

### City
Ada berapa lokasi kota wisata dalam dataset?
"""

gradient_blue = LinearSegmentedColormap.from_list("gradient_blue", ["#add8e6", "#00008b"])
plt.figure(figsize=(8, 5))  # Ubah figsize untuk tampilan yang lebih baik
city_counts = place_tourism['City'].value_counts().sort_values(ascending=True)

# Buat vertical bar plot dengan gradient blue color
bars = plt.bar(
    city_counts.index,  # Kota di sumbu x
    city_counts.values,  # Jumlah di sumbu y
    color=gradient_blue([i / len(city_counts) for i in range(len(city_counts))])
)

for bar in bars:
    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height(),
             str(int(bar.get_height())),
             va='bottom',
             ha='center',
             fontsize=9,
             color='black')

plt.suptitle('Distribusi Data Kota Tempat Wisata', fontsize=16)
#plt.xlabel('Kota', fontsize=12)  # Label sumbu x
plt.ylabel('Jumlah', fontsize=12)  # Label sumbu y

# Rotasi label sumbu x agar mudah dibaca
plt.xticks(rotation=45, ha='right')

# Sesuaikan layout dan tampilkan
plt.tight_layout()
plt.show()

"""**Yogyakarta** memiliki jumlah terbanyak sebesar 126, selisih 2 dengan **Bandung** yang sebesar 124. Kemudian **Surabaya** menjadi yang terakhir sebesar 46 saja

### Pesebaran harga
Persebaran Data harga Pada wisata
"""

plt.figure(figsize=(5, 3))
sns.histplot(place_tourism['Price'], kde=True)
plt.title('Distribution of Price')
plt.xlabel('Price')
plt.ylabel('Frequency')
plt.show()

"""Sebagian besar tempat wisata memiliki harga yang rendah atau gratis. Hal ini terlihat dari tingginya frekuensi pada rentang harga 0 hingga sekitar 25000.

### Rating
"""

gradient_blue = LinearSegmentedColormap.from_list("gradient_blue", ["#add8e6", "#00008b"])
plt.figure(figsize=(6, 4))
rating_counts = place_tourism['Rating'].value_counts().sort_values(ascending=True)

# Buat vertical bar plot dengan gradient blue color
bars = plt.bar(
    rating_counts.index,
    rating_counts.values,
    color=gradient_blue([i / len(rating_counts) for i in range(len(rating_counts))])
)

for bar in bars:
    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height(),
             str(int(bar.get_height())),
             va='bottom',
             ha='center',
             fontsize=9,
             color='black')

plt.suptitle('Distribusi Data Rating Tempat Wisata', fontsize=16)
plt.xlabel('Rating', fontsize=12)  # Label sumbu x
plt.ylabel('Jumlah', fontsize=12)  # Label sumbu y

# Rotasi label sumbu x agar mudah dibaca
plt.xticks(rotation=45, ha='right')

# Sesuaikan layout dan tampilkan
plt.tight_layout()
plt.show()

import seaborn as sns

sns.histplot(place_tourism['Rating'], kde=True)
plt.title('Distribution of Rating')
plt.xlabel('Rating')
plt.ylabel('Frequency')
plt.show()

sns.boxplot(x='Rating', data=place_tourism)
plt.title('Distribution of Rating')
plt.show()

"""## Univariate EDA on **Rating**"""

rating

plt.figure(figsize=(8, 3))
sns.histplot(place_tourism['Rating'], kde=True, bins=20, color='skyblue', label='Histogram')
sns.kdeplot(place_tourism['Rating'], color='red', linewidth=2, label='KDE')
plt.title('Distribusi Rating Tempat Wisata dengan KDE', fontsize=10)
plt.xlabel('Rating', fontsize=9)
plt.ylabel('Frequency/Density', fontsize=9)
plt.legend()
plt.show()

"""Berdasarkan grafik distribusi rating tempat wisata dengan KDE, terlihat bahwa sebagian besar tempat wisata mendapatkan rating di rentang 4.0 hingga 4.5. Hal ini menunjukkan bahwa mayoritas tempat wisata di dataset tersebut memiliki kualitas yang baik dan memuaskan bagi pengunjung. Puncak distribusi KDE berada di sekitar rating 4.2, yang mengindikasikan bahwa rating tersebut merupakan rating yang paling umum diberikan oleh pengguna. Selain itu, distribusi rating cenderung condong ke kanan (right-skewed), yang berarti terdapat beberapa tempat wisata dengan rating yang sangat tinggi, meskipun jumlahnya relatif sedikit. Secara keseluruhan, distribusi rating tempat wisata menunjukkan bahwa mayoritas tempat wisata memiliki kualitas yang baik dan memuaskan bagi pengunjung, dengan beberapa tempat wisata yang memiliki rating sangat tinggi.

## Univariate EDA on **User**
"""

user

"""### Location
Analisis persebaran lokasi
"""

gradient_blue = LinearSegmentedColormap.from_list("gradient_blue", ["#add8e6", "#00008b"])
plt.figure(figsize=(9, 7))
location_counts = user['Location'].value_counts().sort_values(ascending=True)

# Buat horizontal bar plot dengan gradient blue color
bars = plt.barh(
    location_counts.index,
    location_counts.values,
    color=gradient_blue([i / len(location_counts) for i in range(len(location_counts))])
)

for bar in bars:
    plt.text(bar.get_width(), bar.get_y() + bar.get_height() / 2,
             str(int(bar.get_width())),
             va='center',
             ha='left',
             fontsize=9,
             color='black')

plt.suptitle('Distribusi Data Lokasi Wisata', fontsize=16)
plt.xlabel('Jumlah', fontsize=12)
#plt.ylabel('Lokasi', fontsize=12)

# Sesuaikan layout dan tampilkan
plt.tight_layout()
plt.show()

"""Berdasarkan visualisasi data, terlihat bahwa lokasi wisata terbanyak berada di Jakarta, diikuti oleh Bogor dan Bandung. Jumlah lokasi wisata di Jakarta jauh lebih banyak dibandingkan dengan kota-kota lainnya dalam dataset. Sebaliknya, lokasi wisata paling sedikit berada di Yogyakarta, Surabaya, dan Malang. Hal ini mengindikasikan bahwa Jakarta merupakan pusat wisata yang populer, sementara kota-kota seperti Yogyakarta, Surabaya, dan Malang mungkin memiliki daya tarik wisata yang lebih sedikit atau kurang dikenal oleh pengguna dalam dataset.

### Age

Rentang usia pengunjung
"""

# --- Variable, Color & Plot Size ---
var = 'Age'
fig = plt.figure(figsize=(12, 12))

# --- Skewness & Kurtosis ---
print('\033[1m'+'.: Age Column Skewness & Kurtosis :.'+'\033[0m')
print('*' * 40)
print('Skewness:'+'\033[1m {:.3f}'.format(user[var].skew(axis=0, skipna=True)))
print('\033[0m'+'Kurtosis:'+'\033[1m {:.3f}'.format(user[var].kurt(axis=0, skipna=True)))
print('\n')

# --- General Title ---
fig.suptitle('Distribusi Usia Pengunjung',
             fontweight='bold',
             fontsize=16,
             fontfamily='sans-serif')
fig.subplots_adjust(top=0.9)

# --- Histogram ---
ax_1 = fig.add_subplot(2, 2, 2)
plt.title('Histogram Plot',
          fontweight='bold',
          fontsize=14,
          fontfamily='sans-serif')

sns.histplot(data=user,
             x=var,
             kde=True)

plt.xlabel('Total',
           fontweight='regular',
           fontsize=11,
           fontfamily='sans-serif')

plt.ylabel('Age',
           fontweight='regular',
           fontsize=11,
           fontfamily='sans-serif')

# --- Q-Q Plot ---
ax_2 = fig.add_subplot(2, 2, 4)
plt.title('Q-Q Plot',
          fontweight='bold',
          fontsize=14,
          fontfamily='sans-serif')

sm.qqplot(user[var],
       fit=True,
       line='45',
       ax=ax_2,
       color='#BA1141')

plt.xlabel('Theoretical Quantiles',
           fontweight='regular',
           fontsize=11,
           fontfamily='sans-serif')

plt.ylabel('Sample Quantiles',
           fontweight='regular',
           fontsize=11,
           fontfamily='sans-serif')

# --- Box Plot ---
ax_3 = fig.add_subplot(1, 2, 1)
plt.title('Box Plot', fontweight='bold', fontsize=14, fontfamily='sans-serif')
sns.boxplot(data=user, y=var, boxprops=dict(alpha=0.8), linewidth=1.5)
plt.ylabel('Age', fontweight='regular', fontsize=11, fontfamily='sans-serif')

plt.show()

"""Berdasarkan analisis distribusi usia pengunjung, diketahui bahwa mayoritas pengunjung berusia antara 20 hingga 30 tahun. Distribusi data usia cenderung terdistribusi normal dengan sedikit kemiringan ke kanan (right-skewed), mengindikasikan adanya beberapa pengunjung dengan usia yang lebih tua. Meskipun terdapat outlier pada data usia, namun secara umum dapat disimpulkan bahwa tempat wisata tersebut lebih banyak dikunjungi oleh kalangan muda. Hal ini dapat menjadi pertimbangan dalam strategi pemasaran dan pengembangan produk wisata yang disesuaikan dengan preferensi dan kebutuhan pengunjung dari rentang usia tersebut.

# ⭐ Data Preparation : Content-Based Filtering⭐

Pada tahap ini, dataset place_tourism dan rating akan digabungkan untuk pemodelan content-based filtering. sehingga hanya beberapa kolom saja yang akan digunakan dalam dataset

## Feature Selection for Place_tourism
"""

place_tourism.drop(['Rating','Time_Minutes','Coordinate','Lat','Long'],axis=1,inplace=True)
place_tourism.head()

"""## Menggabungkan Data Place_Tourism dengan Rating"""

merged_data = pd.merge(rating.groupby('Place_Id')['Place_Ratings'].mean(),
                       place_tourism,
                       on='Place_Id')
merged_data

# Membuat Copy Data
data_content_based = merged_data.copy()
data_content_based

"""## Text Processing

Menyesuaikan data untuk persiapan content-based Filtering
"""

stem = StemmerFactory().create_stemmer()
stopword = StopWordRemoverFactory().create_stop_word_remover()

def preprocessing(data):
    data = data.lower()
    data = stem.stem(data)
    data = stopword.remove(data)
    return data

# Menyatukan Deskripsi dan Kategori ke dalam kolom Pattern
data_content_based['Pattern'] = data_content_based['Description'] + ' ' + data_content_based['Category']
# Menghilangkan kolom Price, Place Rating, Deskripsi, dan City
data_content_based.drop(['Price','Place_Ratings','Description','City'],axis=1,inplace=True)

data_content_based

# Menerapkan Function
data_content_based['Pattern'] = data_content_based['Pattern'].apply(preprocessing)
data_content_based

"""# ⭐ Modeling Content-Based Filtering⭐

## TF-IDF
"""

from sklearn.feature_extraction.text import TfidfVectorizer

# Inisialisasi TfidfVectorizer
tfidf = TfidfVectorizer()

# Melakukan perhitungan idf pada data cuisine
tfidf.fit(data_content_based['Pattern'])

# Mapping array dari fitur index integer ke fitur nama
tfidf.get_feature_names_out()

# Melakukan fit lalu ditransformasikan ke bentuk matrix
tfidf_matrix = tfidf.fit_transform(data_content_based['Pattern'])

# Melihat ukuran matrix tfidf
tfidf_matrix.shape

# Mengubah vektor tf-idf dalam bentuk matriks dengan fungsi todense()
tfidf_matrix.todense()

# Membuat dataframe untuk melihat tf-idf matrix

pd.DataFrame(
    tfidf_matrix.todense(),
    columns=tfidf.get_feature_names_out(),
    index=data_content_based.Pattern
).sample(22, axis=1).sample(10, axis=0)

"""## Cosine Similarity"""

cosine_sim = cosine_similarity(tfidf_matrix)
cosine_sim

# Membuat dataframe dari variabel cosine_sim dengan baris dan kolom berupa nama tempat wisata
cosine_sim_df = pd.DataFrame(cosine_sim,
                             index=data_content_based['Place_Name'],
                             columns=data_content_based['Place_Name'])

print('Shape:', cosine_sim_df.shape)

# Melihat similarity matrix pada setiap tempat wisata
cosine_sim_df.sample(5, axis=1).sample(10, axis=0)

"""## Develop Content-Based Filtering Recommender System and Get Recommendation"""

def recommend_by_content_based_filtering(nama_tempat, similarity_data=cosine_sim_df, items_data=data_content_based[['Place_Name','Pattern','Category']], k=10):
    """
    Recommends items using content-based filtering and returns a DataFrame.

    Args:
        nama_tempat (str): The name of the item to find recommendations for.
        similarity_data (DataFrame): The DataFrame containing the similarity matrix.
        items_data (DataFrame): The DataFrame containing the item data.
        k (int, optional): The number of recommendations to generate. Defaults to 10.

    Returns:
        DataFrame: A DataFrame containing the recommendations, or an empty DataFrame if the item is not found.
    """

    # Find the index of the given item
    filtered_items = items_data[items_data['Place_Name'] == nama_tempat]

    # Check if the item exists in the dataset
    if filtered_items.empty:
        print(f"Item '{nama_tempat}' not found in the dataset.")
        return pd.DataFrame()  # Return an empty DataFrame if item not found

    nama_tempat_index = filtered_items.index[0]

    # Get the similarity scores for the given item
    similarity_scores = similarity_data.iloc[nama_tempat_index]

    # Sort the items by similarity score in descending order
    nama_tempat_list = similarity_scores.sort_values(ascending=False).index[1:k + 1]

    # Create a DataFrame containing the recommendations
    recommended_items = items_data[items_data['Place_Name'].isin(nama_tempat_list)][['Place_Name', 'Category']]
    recommended_items['similarity_score'] = similarity_scores[nama_tempat_list].values

    return recommended_items

# Skenario 1 : Candi Prambanan
recommend_by_content_based_filtering('Candi Prambanan')

# Case 2 : Museum Basoeki Abdullah
recommend_by_content_based_filtering('Museum Basoeki Abdullah')

recommend_by_content_based_filtering('Dunia Fantasi')

"""# ⭐ Modeling Collaborative Filtering⭐

## Data Understanding
"""

data_collaborative_filtering = rating.copy()
data_collaborative_filtering

"""## Data Preparation"""

# Mengubah userID menjadi list tanpa nilai yang sama
user_ids = data_collaborative_filtering['User_Id'].unique().tolist()
print('list userID: ', user_ids)

# Melakukan encoding userID
user_to_user_encoded = {x: i for i, x in enumerate(user_ids)}
print('encoded userID : ', user_to_user_encoded)

# Melakukan proses encoding angka ke ke userID
user_encoded_to_user = {i: x for i, x in enumerate(user_ids)}
print('encoded angka ke userID: ', user_encoded_to_user)

# Mengubah placeID menjadi list tanpa nilai yang sama
place_ids = data_collaborative_filtering['Place_Id'].unique().tolist()

# Melakukan proses encoding placeID
place_to_place_encoded = {x: i for i, x in enumerate(place_ids)}

# Melakukan proses encoding angka ke placeID
place_encoded_to_place = {i: x for i, x in enumerate(place_ids)}

# Mapping userID ke dataframe user
data_collaborative_filtering['user'] = data_collaborative_filtering['User_Id'].map(user_to_user_encoded)

# Mapping placeID ke dataframe
data_collaborative_filtering['place_encoded'] = data_collaborative_filtering['Place_Id'].map(place_to_place_encoded)

# Mendapatkan jumlah user
num_users = len(user_to_user_encoded)
print(num_users)

# Mendapatkan jumlah resto
num_place = len(place_to_place_encoded)
print(num_place)

# Mengubah rating menjadi nilai float
data_collaborative_filtering['Place_Ratings'] = data_collaborative_filtering['Place_Ratings'].values.astype(np.float32)

# Nilai minimum rating
min_rating = min(data_collaborative_filtering['Place_Ratings'])

# Nilai maksimal rating
max_rating = max(data_collaborative_filtering['Place_Ratings'])

print('Number of User: {}, Number of Places: {}, Min Rating: {}, Max Rating: {}'.format(
    num_users, num_place, min_rating, max_rating
))

"""## Spliting data"""

data_collaborative_filtering = data_collaborative_filtering.sample(frac=1, random_state=42)
data_collaborative_filtering

x = data_collaborative_filtering[['user', 'place_encoded']].values

# Membuat variabel y untuk membuat rating dari hasil
y = data_collaborative_filtering['Place_Ratings'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values

# Membagi menjadi 80% data train dan 20% data validasi
train_indices = int(0.8* data_collaborative_filtering.shape[0])
x_train, x_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:]
)

print(x, y)

"""## Model Development and Training"""

class RecommenderNet(tf.keras.Model):

  # Insialisasi fungsi
  def __init__(self, num_users, num_place, embedding_size, **kwargs):
    super(RecommenderNet, self).__init__(**kwargs)
    self.num_users = num_users
    self.num_place = num_place
    self.embedding_size = embedding_size
    self.user_embedding = layers.Embedding( # layer embedding user
        num_users,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.user_bias = layers.Embedding(num_users, 1) # layer embedding user bias
    self.place_embedding = layers.Embedding( # layer embeddings resto
        num_place,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.place_bias = layers.Embedding(num_place, 1) # layer embedding resto bias

  def call(self, inputs):
    user_vector = self.user_embedding(inputs[:,0]) # memanggil layer embedding 1
    user_bias = self.user_bias(inputs[:, 0]) # memanggil layer embedding 2
    place_vector = self.place_embedding(inputs[:, 1]) # memanggil layer embedding 3
    place_bias = self.place_bias(inputs[:, 1]) # memanggil layer embedding 4

    dot_user_place = tf.tensordot(user_vector, place_vector, 2)

    x = dot_user_place + user_bias + place_bias

    return tf.nn.sigmoid(x) # activation sigmoid

model = RecommenderNet(num_users, num_place, 50) # inisialisasi model

# model compile
model.compile(
    loss = tf.keras.losses.MeanSquaredError(),
    optimizer = keras.optimizers.RMSprop(learning_rate=0.0001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

# Memulai training


early_stopping = tf.keras.callbacks.EarlyStopping(
    monitor='val_loss',  # Monitor validation loss
    patience=5,          # Number of epochs with no improvement after which training will be stopped
    restore_best_weights=True # Restore model weights from the epoch with the best value of the monitored quantity
)

history = model.fit(
    x = x_train,
    y = y_train,
    batch_size =32,
    epochs = 50,
    validation_data = (x_val, y_val),
)

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

class RecommenderNetV2(tf.keras.Model):
    def __init__(self, num_users, num_place, embedding_size, **kwargs):
        super(RecommenderNet, self).__init__(**kwargs)
        self.num_users = num_users
        self.num_place = num_place
        self.embedding_size = embedding_size

        self.user_embedding = layers.Embedding(
            num_users,
            embedding_size,
            embeddings_initializer='he_normal',
            embeddings_regularizer=keras.regularizers.l2(1e-6)
        )
        self.user_bias = layers.Embedding(num_users, 1)
        self.place_embedding = layers.Embedding(
            num_place,
            embedding_size,
            embeddings_initializer='he_normal',
            embeddings_regularizer=keras.regularizers.l2(1e-6)
        )
        self.place_bias = layers.Embedding(num_place, 1)

    def call(self, inputs):
        user_vector = self.user_embedding(inputs[:, 0])
        user_bias = self.user_bias(inputs[:, 0])
        place_vector = self.place_embedding(inputs[:, 1])
        place_bias = self.place_bias(inputs[:, 1])

        dot_user_place = tf.tensordot(user_vector, place_vector, 2)

        # Combine the results
        x = dot_user_place + user_bias + place_bias
        x = tf.nn.sigmoid(x)  # Use sigmoid if your target is between 0 and 1

        # Add additional dense layers for more complexity
        x = layers.Dense(64, activation='relu')(x)
        x = layers.Dropout(0.2)(x)  # Dropout for regularization
        x = layers.Dense(32, activation='relu')(x)

        return x  # Return the final output (no activation for regression)

# Initialize and compile the model
model = RecommenderNet(num_users, num_place, 50)

model.compile(
    #loss = tf.keras.losses.CategoricalFocalCrossentropy(),
    loss = tf.keras.losses.CategoricalCrossentropy(),
    optimizer = keras.optimizers.Adamax(learning_rate=0.0001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

# Train the model
early_stopping = keras.callbacks.EarlyStopping(monitor='val_root_mean_squared_error', patience=5)

history = model.fit(
    x=x_train,
    y=y_train,
    batch_size=64,
    epochs=50,
    validation_data=(x_val, y_val),
    callbacks=[early_stopping]
)

# Visualize the training and validation results
plt.plot(history.history['root_mean_squared_error'], label='Train RMSE')
plt.plot(history.history['val_root_mean_squared_error'], label='Validation RMSE')
plt.title('Training and Validation RMSE RecommendNetV2')
plt.xlabel('Epochs')
plt.ylabel('Root Mean Squared Error')
plt.legend()
plt.show()

data_places = place_tourism.copy()

df = rating

# Mengambil sample user
user_id = df.User_Id.sample(1).iloc[0]
places_visited_by_user = df[df.User_Id == user_id]

# Mengubah placeID menjadi list tanpa nilai yang sama
place_ids = data_collaborative_filtering['Place_Id'].unique().tolist()

# Melakukan proses encoding placeID
place_to_place_encoded = {x: i for i, x in enumerate(place_ids)}

# Melakukan proses encoding angka ke placeID
place_encoded_to_place = {i: x for i, x in enumerate(place_ids)}

places_not_visited = data_places[~data_places['Place_Id'].isin(places_visited_by_user.Place_Id.values)]['Place_Id']

place_not_visited = list(set(places_not_visited).intersection(set(place_to_place_encoded.keys())))

places_not_visited = [[place_to_place_encoded.get(x)] for x in places_not_visited]

user_encoder = user_to_user_encoded.get(user_id)

user_place_array = np.hstack(([[user_encoder]] * len(places_not_visited), places_not_visited))

ratings = model.predict(user_place_array).flatten()

print('Showing recommendations for users: {}'.format(user_id))
print('===' * 9)
print('Places with high ratings from user')
print('----' * 8)

# Sort places visited by user by Place_Ratings in descending order
places_visited_by_user = places_visited_by_user.sort_values(by='Place_Ratings', ascending=False)

# Get top 5 place IDs from sorted data
top_places_user = places_visited_by_user.head(5).Place_Id.values

# Filter place_tourism (renamed to data_places) by place IDs
data_places_rows = data_places[data_places['Place_Id'].isin(top_places_user)]

# Iterate through filtered data and print place names
for row in data_places_rows.itertuples():
    print(row.Place_Name)

print('----' * 8)
print('Top 10 place recommendation')
print('----' * 8)

# Get the indices of the top 10 recommendations from the ratings array
top_ratings_indices = ratings.argsort()[-10:][::-1]

# Get the corresponding place IDs from place_encoded_to_place
recommended_place_ids = [place_encoded_to_place.get(i) for i in top_ratings_indices] # Define recommended_place_ids

# Filter data_places by recommended_place_ids
recommended_places = data_places[data_places['Place_Id'].isin(recommended_place_ids)]

# Iterate through filtered data and print place names
for row in recommended_places.itertuples():
    print(row.Place_Name)